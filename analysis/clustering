import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset
df = pd.read_excel('sales.xlsx')  # Replace with your actual file path
df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')

# Initialize a list to store features
features = []

# Extract features for each product
for product in df['product'].unique():
    product_data = df[df['product'] == product].copy()
    product_data = product_data.sort_values('date')
    product_data['day_index'] = (product_data['date'] - product_data['date'].min()).dt.days
    X = product_data[['day_index']]
    y = product_data['total_orders']
    
    # Linear regression for trend
    model = LinearRegression()
    model.fit(X, y)
    slope = model.coef_[0]
    
    # Total sales
    total_sales = y.sum()
    
    # Average monthly sales
    product_data['month'] = product_data['date'].dt.to_period('M')
    avg_monthly_sales = product_data.groupby('month')['total_orders'].sum().mean()
    
    # Sales volatility
    sales_volatility = y.std()
    
    # Zero sales days percentage
    zero_sales_days_pct = (y == 0).sum() / len(y)
    
    # Recent sales average (last 90 days)
    recent_period = product_data['date'].max() - pd.Timedelta(days=90)
    recent_sales_avg = product_data[product_data['date'] >= recent_period]['total_orders'].mean()
    
    # Sales acceleration
    mid_point = product_data['date'].min() + (product_data['date'].max() - product_data['date'].min()) / 2
    first_half_avg = product_data[product_data['date'] <= mid_point]['total_orders'].mean()
    second_half_avg = product_data[product_data['date'] > mid_point]['total_orders'].mean()
    sales_acceleration = second_half_avg - first_half_avg
    
    features.append({
        'product': product,
        'slope': slope,
        'total_sales': total_sales,
        'avg_monthly_sales': avg_monthly_sales,
        'sales_volatility': sales_volatility,
        'zero_sales_days_pct': zero_sales_days_pct,
        'recent_sales_avg': recent_sales_avg,
        'sales_acceleration': sales_acceleration
    })

# Create a DataFrame from the features
features_df = pd.DataFrame(features)

# Prepare data for clustering
X_features = features_df.drop('product', axis=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_features)

# Apply K-Means clustering with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
features_df['cluster'] = kmeans.fit_predict(X_scaled)

# Analyze cluster centers to map clusters to categories
cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=X_features.columns)
cluster_centers['cluster'] = cluster_centers.index

# Determine cluster categories based on slope
cluster_centers['category'] = cluster_centers['slope'].apply(
    lambda x: 'growing' if x > 0.01 else ('decaying' if x < -0.01 else 'obsolete')
)

# Create a mapping from cluster to category
cluster_mapping = cluster_centers.set_index('cluster')['category'].to_dict()

# Map cluster labels to categories
features_df['category'] = features_df['cluster'].map(cluster_mapping)

# Display the clustered products
print(features_df[['product', 'category']])

# Visualization: Bar plot of product counts per category
plt.figure(figsize=(8, 6))
sns.countplot(data=features_df, x='category', order=['growing', 'decaying', 'obsolete'])
plt.title('Number of Products per Category')
plt.xlabel('Category')
plt.ylabel('Number of Products')
plt.show()

# Visualization: Scatter plot of slope vs. total sales
plt.figure(figsize=(10, 6))
sns.scatterplot(data=features_df, x='slope', y='total_sales', hue='category', palette='Set1')
plt.title('Product Clusters: Slope vs. Total Sales')
plt.xlabel('Sales Trend Slope')
plt.ylabel('Total Sales')
plt.legend(title='Category')
plt.show()

